{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognizer\n",
    "\n",
    "In this notebook we work with the famous MNIST digits dataset. Our task will be to use a convolutional neural network to correctly identify digits provided as greyscale images. We will use two different apporaches: one with an image data generator and one without. In the end we find that the apprLet's get started!\n",
    "\n",
    "## Imports\n",
    "Let's first import the standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid') # preferred seaborn aesthetic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is built into the tensorflow module. Let's import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Let's take a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (60000, 28, 28)\n",
      "Shape of test data: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data:', X_train.shape)\n",
    "print('Shape of test data:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set consists of 28 by 28 images with 60 000 images in total. Our test set has 10 000 images. Let's make sure all images in our set are indeed 28 by 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x1c5f211cca0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGoCAYAAAAerAGHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAATd0lEQVR4nO3dbYjd533n4e+4kkcZIxFnHdu7xVvlTe6Ipg/gUurYSr0kaaMmMS2ErQle05Y4BGwQpo82UkOMC6U0u4Q4Ji1UEAWEqQ0GU9jY4I1h+2CWFX6RlPGtlNZrFsdpKtdxndGMPNb0xTlKp6PRnNHRaM78Zq4LBOfpZn73OeP56P8/Z+SppaWlAEAVV016AAC4FMIFQCnCBUApwgVAKcIFQClbKVxLlf689NJLE5/BPu3VXrfdPlmHrRSuUs6cOTPpETbFTtlnYq/b0U7Z504jXACUMrWFfgF5rEHOLp7L1bv0F9g6LuPn0tRGz7IdlQ9Xktz5p3+zkXOsy9zcXGZmZjb96262nbLPxF63o0nt87HP3DLuUuFaB4cqAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQiXACUIlwAlCJcAJQytbS0NOkZkiQnT578epLrJj0HwAT908033/zRSQ+x1W2ZcAHAejhVCEApwgVAKcIFQCnCBUApwgVAKcIFQCnCBUApwgVAKcIFQClbJlzf/va3l5KU+fPSSy9NfAb7tFd73Xb7vCTVfm5u1HOxZcK1uLg46REuyZkzZyY9wqbYKftM7HU72u77rPZzc6NsmXABwHoIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIF0BRS0uX/H9C2fLOLp4b+ZhdmzAHAFfA1NRU7vzTv5n0GBvqsc/cMvIxjrgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKEW4AChFuAAoRbgAKGXXWne21nYnOZZkf5LpJA8neTnJV5IsJjmV5NO993OrrL0+yckkH+m9v7ixYwOwU4064roryene+8Ekh5I8kuRzSR7qvd+WQcw+tnLRMHh/kuTMxo4LwE43KlyPJzm67PpikheSvKu1NpVkb5K3Vln3xxkclb2yEUMCwHlrnirsvb+ZJK21vUmeSHIkyVKSLw8vfz/Jc8vXtNZ+Lcn3eu9Pt9YeWO8gCwsLmZ2dvZTZJ2p+fr7UvOPaKftM7HU7qrbPAwcOXPKaubm5KzDJZM3Ozq75XKwZriRprd2U5Mkkj/beT7TW/jHJwd7737bW7k3yhST3LlvyG0mWWmsfTvLTSY631u7ovb+61teZnp4e60WblFFP7HaxU/aZ2Ot2tBP2OTMzM+kRNtyo12zUhzNuSPJMkvt6788Ob34tyRvDy68kuXX5mt77B5etfy7JZ0dFCwDWa9QR14NJrk1ytLV2/r2ue5I81lpbTHJ2eD2tteNJjvTeX75SwwLAqPe4Dic5vMpdt67y2LtXue32sScDgFX4BWQAShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBK2bXWna213UmOJdmfZDrJw0leTvKVJItJTiX5dO/93Fpreu9PXYHZAdiBRh1x3ZXkdO/9YJJDSR5J8rkkD/Xeb8sgTB9bxxoA2BBrHnEleTzJE8uuLyZ5Icm7WmtTSfYmeWsdawBgQ6wZrt77m0nSWtubQYyOJFlK8uXh5e8neW4da0ZaWFjI7OzspU0/QfPz86XmHddO2Wdir9tRtX0eOHDgktfMzc1dgUkma3Z2ds3nYtQRV1prNyV5MsmjvfcTrbV/THKw9/63rbV7k3whyb1rrVnPoNPT02O9aJMy6ondLnbKPhN73Y52wj5nZmYmPcKGG/Wajfpwxg1JnklyX+/92eHNryV5Y3j5lSS3rmMNAGyIUUdcDya5NsnR1trR4W33JHmstbaY5OzwelprxzM4Lfibq6w51Hs/s9HDA7DzjHqP63CSw6vcdesqj717ePFiawDgsvkFZABKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAEoRLgBKES4AShEuAErZNekBYNJePzOfU6/+IN89uy//8g+n894br8k737Fn0mMBF7FmuFpru5McS7I/yXSSh5O8nOQrSRaTnEry6d77uWVrrkryaJKfSrIwvP/vrsTwcLlePzOfZ771vfz+U9/K/Fvnsmf3VXnojvfnF97/bvGCLWrUqcK7kpzuvR9McijJI0k+l+Sh3vttGcTsYyvW/HKSPb33W5L8XpIvbOjEsIFOvfqDH0YrSebfOpfff+pbOfXqDyY8GXAxo04VPp7kiWXXF5O8kORdrbWpJHuTvLVizW1Jvp4kvffnW2s/s55BFhYWMjs7u66ht4L5+flS845ru+/zu2f3/TBa582/dS7ffWN773u7v67nVdvngQMHLnnN3NzcFZhksmZnZ9d8LtYMV+/9zSRpre3NIGBHkiwl+fLw8veTPLdi2b7h7ee93Vrb1XtfXOtrTU9Pj/WiTcqoJ3a72O77/Jd/OJ09u6/6d/Has/uq3LBvTw6850cnONmVtd1f1/N2wj5nZmYmPcKGG/WajfxUYWvtpiTfSPK13vuJJF9McrD3/r4kx3PhqcA3MjgS++HXGBUtmJT33nhNHrrj/dmze/Cfwvn3uN574zUTngy4mFEfzrghyTNJ7uu9Pzu8+bUM4pQkryS5dcWyv0ryiSR/3lr7uSTf3LhxYWO98x178gvvf3f2X/ez+e4b87lh3x6fKoQtbtR7XA8muTbJ0dba0eFt9yR5rLW2mOTs8Hpaa8czOH34ZJKPtNb+OslUkl+/EoPDRnnnO/bkZ9+zZ3BaaRufHoTtYtR7XIeTHF7lrpVHWem9373s6mcvcy4AWJV/OQOAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BShAuAUoQLgFKEC4BSdq11Z2ttd5JjSfYnmU7ycJJPJblx+JD9SZ7vvd+5Ys1Xh/e9neSe3vuLGzw3ADvUqCOuu5Kc7r0fTHIoySO99zt777cn+ZUkrye5f8WaX0qyq/f+gSQPJfmDDZ0YgB1tzSOuJI8neWLZ9cVllz+f5Eu99++sWHMqya7W2lVJ9iV567KnBIChNcPVe38zSVprezMI2JHh9euTfCgXHm0lyZsZnCZ8Mcl1ST6+nkEWFhYyOzu73rknbn5+vtS849op+0zsdTuqts8DBw5c8pq5ubkrMMlkzc7OrvlcjDriSmvtpiRPJnm0935iePMnk5zovb+9ypL7kzzde39guPZ/tdZ+ovc+v9bXmZ6eHutFm5RRT+x2sVP2mdjrdrQT9jkzMzPpETbcqNds1IczbkjyTJL7eu/PLrvrwxl8UGM1/5x/Oz34WpLdSX5kPcMCwCijjrgeTHJtkqOttaPD2w4laUn+fvkDW2vHMziV+D+SHGut/e8kVyd5sPf+gw2dGoAda9R7XIeTHF7lrh9f5bF3L7v6Xy9zLgBYlV9ABqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqAU4QKgFOECoBThAqCUXWvd2VrbneRYkv1JppM8nORTSW4cPmR/kud773euWPdAkjuSXJ3k0d77n23o1ADsWGuGK8ldSU733v9ba+0/JHmh9/6fk6S1dm2SbyS5f/mC1trtST6Q5NYkM0l+a6OHBmDnGhWux5M8sez64rLLn0/ypd77d1as+cUk30zyZJJ9SX57PYMsLCxkdnZ2PQ/dEubn50vNO66dss/EXrejavs8cODAJa+Zm5u7ApNM1uzs7JrPxZrh6r2/mSSttb0ZBOzI8Pr1ST6UFUdbQ9cl+bEkH0/yniRPtdbe13tfWutrTU9Pj/WiTcqoJ3a72Cn7TOx1O9oJ+5yZmZn0CBtu1Gs28sMZrbWbMjgl+LXe+4nhzZ9McqL3/vYqS04nebr3frb33pPMJ3n3JU0NABexZrhaazckeSbJ7/bejy2768NJ/udFlv1lko+21qZaa/8pyTUZxAwALtuo97geTHJtkqOttaPD2w4laUn+fvkDW2vHkxzpvf9Fa+2DSf5PBmG89yJHZgBwyUa9x3U4yeFV7vrxVR5797LLv3P5owHAhfwCMgClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKXsmvQAAIxnaWkpj33mlkmPsaHOLp7L1bvWPqZyxAVQ1NTU1KRH2HCjopUIFwDFCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClCBcApQgXAKUIFwClTC0tLU16hiTJyZMnv5fk/016DoAJ+qebb775o+t98MmTJ79+KY/fLrZMuABgPZwqBKAU4QKgFOECoBThAqAU4QKgFOECoJRdkx5gq2mt7U5yLMn+JNNJHk7yqSQ3Dh+yP8nzvfc7V6x7IMkdSa5O8mjv/c82aeSxjbPX4ZqvDu97O8k9vfcXN23oMV1kry8n+UqSxSSnkny6935u2Zqrkjya5KeSLAzv/7vNnfzSjLnPC9b03p/a1MHHMM5el629PsnJJB+p8P3Lv+eI60J3JTndez+Y5FCSR3rvd/beb0/yK0leT3L/8gWttduTfCDJrUl+PslNmzjv5bjkvSb5pSS7eu8fSPJQkj/YvHEvywV7TfK5JA/13m/L4Affx1as+eUke3rvtyT5vSRf2LxxxzbOPldbU8E4ez0fvD9JcmYTZ2UDOeK60ONJnlh2fXHZ5c8n+VLv/Tsr1vxikm8meTLJviS/fUUn3Djj7PVUkl3Do5F9Sd66siNumNX2+kKSd7XWppLszYV7uS3J15Ok9/58a+1nNmPQyzTOPtf6PtjKxtlrkvxxBkdlD1zxCbkihGuF3vubSdJa25vBfxRHhtevT/KhXHgEkiTXJfmxJB9P8p4kT7XW3td739L/LMmYe30zg1MzL2aw749vxqyX6yJ7XUry5eHl7yd5bsWyfcPbz3u7tbar975lf7CPs8+LfR9sdePstbX2a0m+13t/enh6n4KcKlxFa+2mJN9I8rXe+4nhzZ9McqL3/vYqS04nebr3frb33pPMJ3n35kx7ecbY6/0Z7PW9Gbz389XW2p7NmfbyrLLXLyY52Ht/X5LjufBU4BsZ/K39vKu2crTOG2OfF/s+2PLG2OtvJPlIa+25JD+d5Hhr7cZQiiOuFVprNyR5Jsl9vfdnl9314Qze/F3NXyY53Fr770n+Y5JrMojZljbmXv85/3b65bUku5P8yBUbcoNcZK+vZRCnJHklg/col/urJJ9I8uettZ/L4HTwljbOPtf4PtjSxtlr7/2Dy9Y/l+SzvfdXr/y0bCT/yO4KrbUvJvnVDE6FnXcoyf9Ncmvv/fVljz2e5Ejv/eXW2h8l+S8ZHMU+2Ht/evOmHs84e83gB8OxDAJ9dZIvVvgb+kX2ejTJH2bw3sjZDD4h+dKyvf7/DD5V+JNJppL8+lb/BNqY+/zNVdYc6r1v6Q8vjLPX3vvLy9Y/l0G4tvRryoWEC4BSvMcFQCnCBUApwgVAKcIFQCnCBUApwgVAKcIFQCn/CpzITVvVNEcBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize variables\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# record the widths and heights of the images in our training set\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    widths.append(X_train[i].shape[0])\n",
    "    heights.append(X_train[i].shape[1])\n",
    "    \n",
    "# create a jointplot of image sizes\n",
    "sns.jointplot(x = widths, y = heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every image in our training set is indeed 28 by 28.\n",
    "\n",
    "Let's take a look at an image in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This number is 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = np.random.randint(0,len(X_train)+1)\n",
    "plt.imshow(X_train[num], cmap = 'binary')\n",
    "print('This number is', y_train[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding the Labels\n",
    "\n",
    "Before we create our model we should one-hot encode the labels. Indeed, currently the labels are the digits themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Image Data Generator\n",
    "\n",
    "We use an image data generator to increase the robustness of our classifier. By applying random transformations to our images we generalize our model, thereby reducing overfitting.\n",
    "\n",
    "Before we use an image data generator we will need to rescale and reshape our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale data\n",
    "X_test = X_test / X_train.max()\n",
    "X_train = X_train / X_train.max()\n",
    "\n",
    "# reshape data\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our image data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rotation_range = 10, # rotate by up to 10 degrees\n",
    "                            width_shift_range = 0.05, # shift width by up to 5% \n",
    "                            height_shift_range = 0.05, # shift height by up to 5%\n",
    "                            shear_range = 10, # shear by up to 10 degrees\n",
    "                            zoom_range = 0.05, # zoom by up to 5%\n",
    "                            horizontal_flip = False, # do not flip images horizontally\n",
    "                            vertical_flip = False) # do not flip images vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how our image data generator changes our images. First we need to rescale and reshape our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image After Transformation')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a random number\n",
    "num = np.random.randint(0, len(X_train) + 1)\n",
    "\n",
    "# display an image both before and after a transformation\n",
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "# plot image before transformation\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[num], cmap = 'binary')\n",
    "plt.title('Image Before Transformation')\n",
    "\n",
    "# plot image after transformation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_gen.random_transform(X_train[num]), cmap = 'binary')\n",
    "plt.title('Image After Transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with an Image Data Generator\n",
    "\n",
    "Now let's create our model to recognize digits in the test set. We use a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1500/1500 [==============================] - 28s 18ms/step - loss: 0.5407 - accuracy: 0.8276 - val_loss: 0.0976 - val_accuracy: 0.9693\n",
      "Epoch 2/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.1159 - accuracy: 0.9639 - val_loss: 0.0567 - val_accuracy: 0.9813\n",
      "Epoch 3/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0834 - accuracy: 0.9748 - val_loss: 0.0562 - val_accuracy: 0.9818\n",
      "Epoch 4/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0673 - accuracy: 0.9793 - val_loss: 0.0455 - val_accuracy: 0.9854\n",
      "Epoch 5/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0615 - accuracy: 0.9810 - val_loss: 0.0451 - val_accuracy: 0.9872\n",
      "Epoch 6/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0572 - accuracy: 0.9825 - val_loss: 0.0444 - val_accuracy: 0.9862\n",
      "Epoch 7/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0535 - accuracy: 0.9843 - val_loss: 0.0401 - val_accuracy: 0.9877\n",
      "Epoch 8/1000\n",
      "1500/1500 [==============================] - 29s 20ms/step - loss: 0.0447 - accuracy: 0.9869 - val_loss: 0.0438 - val_accuracy: 0.9869\n",
      "Epoch 9/1000\n",
      "1500/1500 [==============================] - 28s 19ms/step - loss: 0.0453 - accuracy: 0.9874 - val_loss: 0.0404 - val_accuracy: 0.9883\n",
      "Epoch 10/1000\n",
      "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.0322 - val_accuracy: 0.9898\n",
      "Epoch 11/1000\n",
      "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0423 - accuracy: 0.9871 - val_loss: 0.0363 - val_accuracy: 0.9888\n",
      "Epoch 12/1000\n",
      "1500/1500 [==============================] - 30s 20ms/step - loss: 0.0387 - accuracy: 0.9887 - val_loss: 0.0341 - val_accuracy: 0.9901\n",
      "Epoch 13/1000\n",
      "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0413 - accuracy: 0.9870 - val_loss: 0.0379 - val_accuracy: 0.9893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1185\n",
      "           1       1.00      0.99      1.00      1348\n",
      "           2       0.99      1.00      0.99      1192\n",
      "           3       1.00      1.00      1.00      1226\n",
      "           4       0.99      0.99      0.99      1168\n",
      "           5       0.99      0.99      0.99      1084\n",
      "           6       0.99      1.00      0.99      1184\n",
      "           7       0.99      1.00      0.99      1253\n",
      "           8       0.99      0.99      0.99      1170\n",
      "           9       0.99      0.99      0.99      1190\n",
      "\n",
      "    accuracy                           0.99     12000\n",
      "   macro avg       0.99      0.99      0.99     12000\n",
      "weighted avg       0.99      0.99      0.99     12000\n",
      "\n",
      "[[1178    1    3    0    0    0    2    0    1    0]\n",
      " [   0 1339    1    0    1    0    1    6    0    0]\n",
      " [   0    0 1187    0    0    0    0    3    1    1]\n",
      " [   0    0    2 1221    0    2    0    1    0    0]\n",
      " [   0    2    0    0 1161    0    0    1    1    3]\n",
      " [   0    0    0    3    0 1071    4    1    2    3]\n",
      " [   0    0    0    0    1    2 1181    0    0    0]\n",
      " [   0    0    1    1    0    0    0 1247    1    3]\n",
      " [   2    0    1    2    0    1    2    1 1160    1]\n",
      " [   0    0    0    0    6    1    0    3    4 1176]]\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# perform a train-test split\n",
    "X_trainer, X_tester, y_trainer, y_tester = train_test_split(X_train, y_train, test_size = 0.2, random_state = 18,\n",
    "                                                            stratify = y_train)\n",
    "\n",
    "# one-hot encode y_trainer and y_tester\n",
    "y_trainer_cat = to_categorical(y_trainer)\n",
    "y_tester_cat = to_categorical(y_tester)\n",
    "\n",
    "# apply the image data generator to the training and validation sets\n",
    "image_gen_train = image_gen.flow(x = X_trainer, y = y_trainer_cat, shuffle = True)\n",
    "image_gen_test = image_gen.flow(x = X_tester, y = y_tester_cat, shuffle = True)\n",
    "\n",
    "# fit the model\n",
    "stop = EarlyStopping(patience = 3, restore_best_weights = True)\n",
    "model.fit(x = image_gen_train, epochs = 1000, validation_data = image_gen_test, callbacks = [stop])\n",
    "\n",
    "# make predictions\n",
    "pred_probs = model.predict(X_tester)\n",
    "pred = [np.argmax(i) for i in pred_probs]\n",
    "\n",
    "# print metrics\n",
    "print(classification_report(y_tester, pred))\n",
    "print(confusion_matrix(y_tester, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network seems to work rather well. How many images were misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified images in validation set: 79\n",
      "Total number of images in validation set: 12000\n",
      "Percentage of misclassified images: 0.6583333333333333%\n"
     ]
    }
   ],
   "source": [
    "print('Number of misclassified images in validation set:', np.sum([y_tester != pred]))\n",
    "print('Total number of images in validation set:', len(X_tester))\n",
    "print('Percentage of misclassified images:', str(100 * np.sum([y_tester != pred]) / len(X_tester))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs quite well so we will use it to predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.5109 - accuracy: 0.8350\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1066 - accuracy: 0.9674\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0748 - accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0637 - accuracy: 0.9803\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0521 - accuracy: 0.9839\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0523 - accuracy: 0.9837\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0478 - accuracy: 0.9856\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0526 - accuracy: 0.98480s - loss: 0.0527 - \n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0388 - accuracy: 0.9880\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0389 - accuracy: 0.9881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       980\n",
      "           1       1.00      1.00      1.00      1135\n",
      "           2       1.00      0.99      1.00      1032\n",
      "           3       0.99      1.00      0.99      1010\n",
      "           4       1.00      0.99      0.99       982\n",
      "           5       1.00      0.99      0.99       892\n",
      "           6       1.00      1.00      1.00       958\n",
      "           7       0.99      1.00      0.99      1028\n",
      "           8       1.00      1.00      1.00       974\n",
      "           9       0.99      0.99      0.99      1009\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n",
      "[[ 979    0    0    0    0    0    0    1    0    0]\n",
      " [   0 1133    0    0    0    0    0    2    0    0]\n",
      " [   0    0 1026    1    0    0    0    5    0    0]\n",
      " [   0    1    1 1005    0    2    0    0    1    0]\n",
      " [   0    0    0    0  975    0    0    1    0    6]\n",
      " [   1    0    0    5    0  884    2    0    0    0]\n",
      " [   1    1    0    0    1    1  954    0    0    0]\n",
      " [   0    1    1    0    0    0    0 1025    0    1]\n",
      " [   1    0    2    0    0    0    0    0  971    0]\n",
      " [   0    0    0    0    3    1    0    3    0 1002]]\n",
      "Number of misclassified images in test set: 46\n",
      "Total number of images in test set: 10000\n",
      "Percentage of misclassified images: 0.46%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# apply the image data generator to the training set\n",
    "image_gen_train = image_gen.flow(x = X_train, y = y_train_cat, shuffle = True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(x = image_gen_train, epochs = 10)\n",
    "\n",
    "# make predictions\n",
    "pred_probs = model.predict(X_test)\n",
    "pred = [np.argmax(i) for i in pred_probs]\n",
    "\n",
    "# print metrics\n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('Number of misclassified images in test set:', np.sum([y_test != pred]))\n",
    "print('Total number of images in test set:', len(X_test))\n",
    "print('Percentage of misclassified images:', str(100 * np.sum([y_test != pred]) / len(X_test))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional neural network with an image data generator works rather well! Let's see how well a convolutional neural network works without an image data generator.\n",
    "\n",
    "## Predictions without an Image Data Generator\n",
    "\n",
    "Since the MNIST dataset is already rather large and clean it is entirely possible that a convolutional neural network will perform well, and perhaps better, without a data image generator. Let's see if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1500/1500 [==============================] - 27s 18ms/step - loss: 0.4274 - accuracy: 0.8629 - val_loss: 0.0655 - val_accuracy: 0.9795\n",
      "Epoch 2/1000\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0805 - accuracy: 0.9757 - val_loss: 0.0547 - val_accuracy: 0.9818\n",
      "Epoch 3/1000\n",
      "1500/1500 [==============================] - 25s 16ms/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9862\n",
      "Epoch 4/1000\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0417 - accuracy: 0.9873 - val_loss: 0.0367 - val_accuracy: 0.9886\n",
      "Epoch 5/1000\n",
      "1500/1500 [==============================] - 25s 16ms/step - loss: 0.0331 - accuracy: 0.9898 - val_loss: 0.0388 - val_accuracy: 0.9873\n",
      "Epoch 6/1000\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0300 - accuracy: 0.9908 - val_loss: 0.0311 - val_accuracy: 0.9902\n",
      "Epoch 7/1000\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0327 - val_accuracy: 0.9907\n",
      "Epoch 8/1000\n",
      "1500/1500 [==============================] - 25s 16ms/step - loss: 0.0185 - accuracy: 0.9932 - val_loss: 0.0371 - val_accuracy: 0.9902\n",
      "Epoch 9/1000\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0200 - accuracy: 0.9937 - val_loss: 0.0326 - val_accuracy: 0.9909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1185\n",
      "           1       0.99      1.00      0.99      1348\n",
      "           2       0.98      0.99      0.99      1192\n",
      "           3       0.99      0.99      0.99      1226\n",
      "           4       0.99      0.99      0.99      1168\n",
      "           5       0.98      0.99      0.99      1084\n",
      "           6       0.99      1.00      0.99      1184\n",
      "           7       0.99      0.99      0.99      1253\n",
      "           8       0.99      0.98      0.99      1170\n",
      "           9       0.99      0.98      0.98      1190\n",
      "\n",
      "    accuracy                           0.99     12000\n",
      "   macro avg       0.99      0.99      0.99     12000\n",
      "weighted avg       0.99      0.99      0.99     12000\n",
      "\n",
      "[[1182    1    0    0    0    0    1    0    1    0]\n",
      " [   0 1345    0    0    0    0    0    3    0    0]\n",
      " [   0    4 1180    2    0    0    0    4    2    0]\n",
      " [   0    1    4 1210    0    7    0    2    0    2]\n",
      " [   0    2    0    0 1160    0    1    0    0    5]\n",
      " [   1    2    0    2    0 1070    4    0    3    2]\n",
      " [   0    1    0    0    1    3 1179    0    0    0]\n",
      " [   0    1   10    0    0    0    0 1242    0    0]\n",
      " [   2    3    5    3    0    3    2    1 1147    4]\n",
      " [   0    0    0    0    7    4    0    7    5 1167]]\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# perform a train-test split\n",
    "X_trainer, X_tester, y_trainer, y_tester = train_test_split(X_train, y_train, test_size = 0.2, random_state = 18,\n",
    "                                                            stratify = y_train)\n",
    "\n",
    "# one-hot encode y_trainer and y_tester\n",
    "y_trainer_cat = to_categorical(y_trainer)\n",
    "y_tester_cat = to_categorical(y_tester)\n",
    "\n",
    "# fit the model\n",
    "stop = EarlyStopping(patience = 3, restore_best_weights = True)\n",
    "model.fit(x = X_trainer, y = y_trainer_cat, epochs = 1000, validation_data = (X_tester, y_tester_cat), callbacks = [stop])\n",
    "\n",
    "# make predictions\n",
    "pred_probs_no_gen = model.predict(X_tester)\n",
    "pred_no_gen = [np.argmax(i) for i in pred_probs_no_gen]\n",
    "\n",
    "# print metrics\n",
    "print(classification_report(y_tester, pred_no_gen))\n",
    "print(confusion_matrix(y_tester, pred_no_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many images were misclassified in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified images in validation set: 118\n",
      "Total number of images in validation set: 12000\n",
      "Percentage of misclassified images: 0.9833333333333333%\n"
     ]
    }
   ],
   "source": [
    "print('Number of misclassified images in validation set:', np.sum([y_tester != pred_no_gen]))\n",
    "print('Total number of images in validation set:', len(X_tester))\n",
    "print('Percentage of misclassified images:', str(100 * np.sum([y_tester != pred_no_gen]) / len(X_tester))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the validation set the convolutional neural network without an image data generator does not perform as well as the neural network with said generator. Perhaps the story is different on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3893 - accuracy: 0.8777\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0667 - accuracy: 0.9794\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0493 - accuracy: 0.9848\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0371 - accuracy: 0.9890\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0315 - accuracy: 0.9902\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0254 - accuracy: 0.9920\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       980\n",
      "           1       0.99      1.00      1.00      1135\n",
      "           2       1.00      0.99      0.99      1032\n",
      "           3       1.00      0.99      0.99      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.99      0.99      0.99      1028\n",
      "           8       0.99      0.99      0.99       974\n",
      "           9       0.99      0.98      0.99      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "[[ 975    0    0    0    0    0    4    1    0    0]\n",
      " [   0 1134    0    0    1    0    0    0    0    0]\n",
      " [   0    1 1025    0    1    0    0    5    0    0]\n",
      " [   0    1    1  998    0    6    0    2    2    0]\n",
      " [   0    0    0    0  977    0    3    0    1    1]\n",
      " [   1    0    0    4    0  884    1    1    0    1]\n",
      " [   2    1    0    0    1    1  951    0    2    0]\n",
      " [   0    3    3    0    0    1    0 1019    1    1]\n",
      " [   1    0    1    1    0    0    1    0  968    2]\n",
      " [   2    0    0    0    9    4    0    4    2  988]]\n",
      "Number of misclassified images in test set: 81\n",
      "Total number of images in test set: 10000\n",
      "Percentage of misclassified images: 0.81%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(x = X_train, y = y_train_cat, epochs = 6)\n",
    "\n",
    "# make predictions\n",
    "pred_probs_no_gen = model.predict(X_test)\n",
    "pred_no_gen = [np.argmax(i) for i in pred_probs_no_gen]\n",
    "\n",
    "# print metrics\n",
    "print(classification_report(y_test, pred_no_gen))\n",
    "print(confusion_matrix(y_test, pred_no_gen))\n",
    "print('Number of misclassified images in test set:', np.sum([y_test != pred_no_gen]))\n",
    "print('Total number of images in test set:', len(X_test))\n",
    "print('Percentage of misclassified images:', str(100 * np.sum([y_test != pred_no_gen]) / len(X_test))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result is quite good, the convolutional neural network with the image data generator still performed better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
